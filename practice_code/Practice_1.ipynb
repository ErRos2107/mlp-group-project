{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1758150/miniconda3/envs/mlp/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 1234\n",
    "dropout = 0.25\n",
    "rnd = tf.set_random_seed(seed)\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, 784], 'inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, 47], 'targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine Layer Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 1.77\n",
      "End of epoch 2: running error average = 1.47\n",
      "End of epoch 3: running error average = 1.43\n",
      "End of epoch 4: running error average = 1.41\n",
      "End of epoch 5: running error average = 1.38\n",
      "End of epoch 6: running error average = 1.38\n",
      "End of epoch 7: running error average = 1.37\n",
      "End of epoch 8: running error average = 1.37\n",
      "End of epoch 9: running error average = 1.36\n",
      "End of epoch 10: running error average = 1.36\n"
     ]
    }
   ],
   "source": [
    "do1 = tf.layers.dropout(inputs, rate=dropout, seed=rnd, training=True)\n",
    "w1 = tf.get_variable(\"weights1\", shape=[784,100], initializer=tf.glorot_uniform_initializer())\n",
    "b1 = tf.get_variable(\"bias1\", shape=[100], initializer=tf.glorot_uniform_initializer())\n",
    "relu1 = tf.nn.relu(tf.matmul(do1, w1)+b1)\n",
    "\n",
    "do2 = tf.layers.dropout(relu1, rate=dropout, seed=rnd, training=True)\n",
    "w2 = tf.get_variable(\"weights2\", shape=[100,100], initializer=tf.glorot_uniform_initializer())\n",
    "b2 = tf.get_variable(\"bias2\", shape=[100], initializer=tf.glorot_uniform_initializer())\n",
    "relu2 = tf.nn.relu(tf.matmul(do2, w2)+b2)\n",
    "\n",
    "do3 = tf.layers.dropout(relu2, rate=dropout, seed=rnd, training=True)\n",
    "w3 = tf.get_variable(\"weights3\", shape=[100,47], initializer=tf.glorot_uniform_initializer())\n",
    "b3 = tf.get_variable(\"bias3\", shape=[47], initializer=tf.glorot_uniform_initializer())\n",
    "outputs = tf.matmul(do3, w3)+b3\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                        logits = outputs, labels =targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(outputs,1),\\\n",
    "                                         tf.argmax(targets,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = 0.5).minimize(error)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "import data_providers as data_providers\n",
    "\n",
    "train_data = data_providers.EMNISTDataProvider('train', batch_size=50,\\\n",
    "                                              flatten=True, one_hot=True)\n",
    "valid_data = data_providers.EMNISTDataProvider('valid', batch_size=50,\\\n",
    "                                              flatten=True, one_hot=True)\n",
    "\n",
    "num_epoch = 10\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    running_error=0\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error = sess.run([train_step, error], \\\n",
    "                                 feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "    running_error /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e+1, running_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCN Structure from TF tutorial (but dataset is EMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 0.93\n",
      "End of epoch 2: running error average = 0.50\n",
      "End of epoch 3: running error average = 0.44\n",
      "End of epoch 4: running error average = 0.40\n",
      "End of epoch 5: running error average = 0.38\n",
      "End of epoch 6: running error average = 0.36\n",
      "End of epoch 7: running error average = 0.34\n",
      "End of epoch 8: running error average = 0.33\n",
      "End of epoch 9: running error average = 0.32\n",
      "End of epoch 10: running error average = 0.31\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.reshape(inputs, [-1,28,28,1])\n",
    "conv1 = tf.layers.conv2d(inputs = input_layer, filters=32,\n",
    "                        kernel_size=[5,5], padding=\"same\",\n",
    "                        activation = tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2],\n",
    "                               strides=2)\n",
    "conv2 = tf.layers.conv2d(inputs = pool1, filters=64,\n",
    "                        kernel_size=[5,5], padding=\"same\",\n",
    "                        activation = tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2],\n",
    "                               strides=2)\n",
    "\n",
    "pool2_flat = tf.reshape(pool2, [-1,7*7*64])\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024,\n",
    "                        activation = tf.nn.relu)\n",
    "dropout = tf.layers.dropout(inputs = dense, rate= 0.4, \n",
    "                           training=True)\n",
    "outputs = tf.layers.dense(inputs=dropout, units=47)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                        logits = outputs, labels =targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(outputs,1),\\\n",
    "                                         tf.argmax(targets,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = 0.5).minimize(error)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "import data_providers as data_providers\n",
    "\n",
    "train_data = data_providers.EMNISTDataProvider('train', batch_size=50,\\\n",
    "                                              flatten=True, one_hot=True)\n",
    "valid_data = data_providers.EMNISTDataProvider('valid', batch_size=50,\\\n",
    "                                              flatten=True, one_hot=True)\n",
    "\n",
    "num_epoch = 10\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    running_error=0\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error = sess.run([train_step, error], \\\n",
    "                                 feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "    running_error /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e+1, running_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-layer CCN Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 1.07\n",
      "End of epoch 2: running error average = 0.73\n",
      "End of epoch 3: running error average = 0.67\n",
      "End of epoch 4: running error average = 1.05\n",
      "End of epoch 5: running error average = 0.95\n",
      "End of epoch 6: running error average = 0.89\n",
      "End of epoch 7: running error average = 0.82\n",
      "End of epoch 8: running error average = 0.69\n",
      "End of epoch 9: running error average = 0.66\n",
      "End of epoch 10: running error average = 0.64\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.reshape(inputs, [-1,28,28,1])\n",
    "conv1 = tf.layers.conv2d(inputs = input_layer, filters=5,\n",
    "                        kernel_size=[5,5], padding=\"valid\",\n",
    "                        activation = tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2],\n",
    "                               strides=2)\n",
    "\n",
    "pool1_flat = tf.reshape(pool1, [-1,5*12*12])\n",
    "outputs = tf.layers.dense(inputs=pool1_flat, units=47)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                        logits = outputs, labels =targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(outputs,1),\\\n",
    "                                         tf.argmax(targets,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = 0.5).minimize(error)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "import data_providers as data_providers\n",
    "\n",
    "train_data = data_providers.EMNISTDataProvider('train', batch_size=100,\\\n",
    "                                              flatten=True, one_hot=True)\n",
    "valid_data = data_providers.EMNISTDataProvider('valid', batch_size=100,\\\n",
    "                                              flatten=True, one_hot=True)\n",
    "\n",
    "num_epoch = 10\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    running_error=0\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error = sess.run([train_step, error], \\\n",
    "                                 feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "    running_error /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e+1, running_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer CCN Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7a627a021851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mrunning_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mrunning_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mrunning_error\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_layer = tf.reshape(inputs, [-1,28,28,1])\n",
    "conv1 = tf.layers.conv2d(inputs = input_layer, filters=5,\n",
    "                        kernel_size=[5,5], padding=\"valid\",\n",
    "                        activation = tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2],\n",
    "                               strides=2)\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs = pool1, filters=10,\n",
    "                        kernel_size=[5,5], padding=\"valid\",\n",
    "                        activation = tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2],\n",
    "                               strides=2)\n",
    "\n",
    "pool2_flat = tf.reshape(pool2, [-1,10*4*4])\n",
    "outputs = tf.layers.dense(inputs=pool2_flat, units=47)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                        logits = outputs, labels =targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(outputs,1),\\\n",
    "                                         tf.argmax(targets,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = 0.5).minimize(error)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "import data_providers as data_providers\n",
    "\n",
    "train_data = data_providers.EMNISTDataProvider('train', batch_size=100,\\\n",
    "                                              flatten=True, one_hot=True)\n",
    "valid_data = data_providers.EMNISTDataProvider('valid', batch_size=100,\\\n",
    "                                              flatten=True, one_hot=True)\n",
    "\n",
    "num_epoch = 10\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    running_error=0\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error = sess.run([train_step, error], \\\n",
    "                                 feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "    running_error /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e+1, running_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data Error = 1.16 Accuracy=0.67\n",
      "Valid data Error = 1.26 Accuracy=0.65\n"
     ]
    }
   ],
   "source": [
    "def get_error_and_accuracy(data):\n",
    "    err = 0\n",
    "    acc = 0\n",
    "    for input_batch, target_batch in data:\n",
    "        e, a = sess.run([error, accuracy], feed_dict={inputs: input_batch, targets: target_batch})    \n",
    "        err+=e\n",
    "        acc+=a\n",
    "    err /= data.num_batches\n",
    "    acc /= data.num_batches\n",
    "    \n",
    "    return err, acc\n",
    "\n",
    "print('Train data Error = {0:.2f} Accuracy={1:.2f}'.format(*get_error_and_accuracy(train_data)))\n",
    "print('Valid data Error = {0:.2f} Accuracy={1:.2f}'.format(*get_error_and_accuracy(valid_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_graph = tf.get_default_graph()\n",
    "print('Number of operations in graph: {0}'.format(len(default_graph.get_operations())))\n",
    "graph = tf.Graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
