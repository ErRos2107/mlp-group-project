{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_provider import ASSISTDataProvider\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '~/Dropbox/mlp-group-project/'\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed(array, dimensions=100, rng=None):\n",
    "    \"\"\"Embed array as a vector from a Standard Normal in dimensions.\n",
    "    \n",
    "    Only the last dimension of the data is affected.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dimensions : int (default=100)\n",
    "        DKT paper embeds one-hot inputs to 100 dims\n",
    "    rng : numpy.random.RandomState (default=None)\n",
    "    \"\"\"\n",
    "    if not rng:\n",
    "        rng = np.random.RandomState()\n",
    "    linear_map = rng.randn(data.shape[-1], dimensions)\n",
    "    return np.dot(data, linear_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmModel:\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LstmModel\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, max_time_steps=973, feature_len=293, n_distinct_questions=146):\n",
    "        \"\"\"Initialise task-specific parameters.\"\"\"\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.feature_len = feature_len\n",
    "        self.n_distinct_questions = n_distinct_questions\n",
    "        \n",
    "        \n",
    "    def build_graph(self, n_hidden_layers=1, n_hidden_units=200, keep_prob=1.0,\n",
    "                    learning_rate=0.01, clip_norm=20.0):\n",
    "        self._build_model(n_hidden_layers, n_hidden_units, keep_prob)\n",
    "        self._build_training(learning_rate, clip_norm)\n",
    "        \n",
    "\n",
    "    def _build_model(self, n_hidden_layers=1, n_hidden_units=200, keep_prob=1.0):\n",
    "        \"\"\"Build a TensorFlow computational graph for an LSTM network.\n",
    "\n",
    "        Model based on \"DKT paper\" (see section 3): \n",
    "            Piech, Chris, et al. \"Deep knowledge tracing.\" \n",
    "            Advances in Neural Information Processing Systems. 2015.\n",
    "            \n",
    "        Implementation based on \"GD paper\" (see section 3): \n",
    "            Xiong, Xiaolu, et al. \"Going Deeper with Deep Knowledge Tracing.\"\n",
    "            EDM. 2016.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_hidden_layers : int (default=1)\n",
    "            A single hidden layer was used in DKT paper\n",
    "        n_hidden_units : int (default=200)\n",
    "            200 hidden units were used in DKT paper\n",
    "        keep_prob : float in [0, 1] (default=1.0)\n",
    "            Probability a unit is kept in dropout layer\n",
    "        \"\"\"\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # data. 'None' means any length batch size accepted\n",
    "        self.inputs = tf.placeholder(\n",
    "            tf.float32, \n",
    "            shape=[self.max_time_steps, None, self.feature_len], \n",
    "            name='inputs')\n",
    "        self.inputs = tf.transpose(self.inputs, [1, 0, 2])\n",
    "        \n",
    "        # 'None' because may have answered any number of questions\n",
    "        self.targets = tf.placeholder(tf.float32, \n",
    "                                      shape=[None], \n",
    "                                      name='targets')\n",
    "        \n",
    "        # int type required for tf.gather function\n",
    "        self.target_ids = tf.placeholder(tf.int32, \n",
    "                                         shape=[None], \n",
    "                                         name='target_ids')\n",
    "\n",
    "        # model. LSTM layer(s) then linear layer (softmax applied in loss)\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden_units)\n",
    "        if keep_prob < 1:\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, keep_prob)\n",
    "        if n_hidden_layers > 1:\n",
    "            cells = [cell for layer in n_hidden_layers]\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "        self.outputs, self.state = tf.nn.dynamic_rnn(cell=cell, \n",
    "                                                     inputs=self.inputs,\n",
    "                                                     dtype=tf.float32)\n",
    "        \n",
    "        sigmoid_w = tf.get_variable(dtype=tf.float32,\n",
    "                                    name=\"sigmoid_w\", \n",
    "                                    shape=[n_hidden_units, \n",
    "                                           self.n_distinct_questions])\n",
    "        sigmoid_b = tf.get_variable(dtype=tf.float32,\n",
    "                                    name=\"sigmoid_b\", \n",
    "                                    shape=[self.n_distinct_questions])\n",
    "        \n",
    "        # reshaping as done in GD paper code\n",
    "        # first dim now batch_size times max_time_steps\n",
    "        self.outputs = tf.reshape(self.outputs, \n",
    "                                  shape=[-1, n_hidden_units])\n",
    "        \n",
    "        logits = tf.matmul(self.outputs, sigmoid_w) + sigmoid_b\n",
    "        logits = tf.reshape(logits, [-1])\n",
    "        self.logits = tf.gather(logits, self.target_ids)\n",
    "        self.predictions = tf.sigmoid(self.logits)\n",
    "\n",
    "        \n",
    "    def _build_training(self, learning_rate=0.001, decay_exp=0.98, clip_norm=20.0):\n",
    "        \"\"\"Define parameters updates, with optional \n",
    "\n",
    "        Applies exponential learning rate decay (optional). See:\n",
    "        https://www.tensorflow.org/versions/r0.12/api_docs/python/train\n",
    "        /decaying_the_learning_rate\n",
    "        \n",
    "        Applies gradient clipping by gloabl norm (optional). See:\n",
    "        https://www.tensorflow.org/versions/r0.12/api_docs/python/train\n",
    "        /gradient_clipping\n",
    "        \"\"\"\n",
    "        loss_per_example = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=self.logits, labels=self.targets)\n",
    "        self.loss = tf.reduce_mean(loss_per_example)\n",
    "        \n",
    "        # track number of batches seen\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        \n",
    "        if decay_exp:\n",
    "            learning_rate = tf.train.exponential_decay(learning_rate=learning_rate, \n",
    "                                                       global_step=self.global_step,\n",
    "                                                       decay_rate=decay_exp,\n",
    "                                                       decay_steps=100,\n",
    "                                                       staircase=True)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        grads, trainable_vars = zip(*optimizer.compute_gradients(self.loss))\n",
    "        if clip_norm:\n",
    "            grads, _ = tf.clip_by_global_norm(grads, clip_norm)\n",
    "        \n",
    "        self.training = optimizer.apply_gradients(zip(grads, trainable_vars), \n",
    "                                                  global_step=self.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = LstmModel()\n",
    "Model.build_graph(n_hidden_units=200, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TrainingSet = ASSISTDataProvider(DATA_DIR, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'first'\n",
    "with tf.Session() as sess:\n",
    "    train_saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(100):         \n",
    "        for inputs, targets, target_ids in TrainingSet:\n",
    "            # ensure shapes and types as model expects\n",
    "            inputs = np.squeeze(np.array(inputs, dtype=np.float32))\n",
    "            inputs = np.transpose(inputs, [1, 0, 2])\n",
    "            targets = np.array(targets, dtype=np.float32)\n",
    "            target_ids = np.array(target_ids, dtype=np.int32)\n",
    "\n",
    "            # Train!\n",
    "            _, loss = sess.run(\n",
    "                [Model.training, Model.loss],\n",
    "                feed_dict={Model.inputs: inputs,\n",
    "                           Model.targets: targets,\n",
    "                           Model.target_ids: target_ids})\n",
    "            \n",
    "        print(loss)\n",
    "        losses.append(loss)\n",
    "            \n",
    "        # save model each epoch\n",
    "        save_path = \"./{}_{}.ckpt\".format(\n",
    "            experiment_name, epoch)\n",
    "        train_saver.save(sess, save_path)            \n",
    "    print(\"Saved model at\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
